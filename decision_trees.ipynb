{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [  # first two columns being features and last one being response\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "header = ['color', 'diameter', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Green', 'Red', 'Yellow'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for row in data:\n",
    "    labels.append(row[0])\n",
    "unique_labels = set(labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impurity: chance of being incorrect if randomly assign a label to an example in the same set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What questions to ask?\n",
    "* When to ask question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a list of questions, we'll iterate over every value for every feature in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, column, value):\n",
    "        self.column = column  # column no (0 for color)\n",
    "        self.value = value  # column value (e.g. Green)\n",
    "        \n",
    "    def match(self, example):\n",
    "        \"\"\"Compare feature value in Question to feature value in example\"\"\"\n",
    "        val = example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"Print Question\"\"\"\n",
    "        condition = '=='\n",
    "        if is_numeric(self.value):\n",
    "            condition = '>='\n",
    "        return 'Is ' + str(header[self.column]) + str(condition) + str(self.value) + '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    \"\"\"For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best question is the one which reduces the uncertainity the most. Gini impurity quantifies how much uncertainity is at a node. Information gain quantifies how much a question reduces the uncertainity.\n",
    "* Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset\n",
    "* Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the purest daughter nodes.\n",
    "\n",
    "To compute Gini impurity for a set of items with $J$ classes, suppose $i âˆˆ { 1 , 2 , . . . , J }$, and let $p_i$ be the fraction of items labeled with class $i$ in the set:  \n",
    "[Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) = $\\sum($P(item with label $i$) * P(mistake in  categorizing that label $i$)$)$\n",
    "$$I_G = 1 - \\sum_{i=1}^J p_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Counts number of each type of example in the dataset\"\"\"\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        label = row[-1]  # last column\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    \"\"\"Calculate Gini impurity for list of rows\"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for label in counts:\n",
    "        prob_label = counts[label] / float(len(rows))\n",
    "        impurity -= prob_label ** 2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_uncertainity):\n",
    "    \"\"\"Information gain: the uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainity - p*gini(left) - (1 - p)*gini(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    info, question = find_best_split(rows)\n",
    "    # base case (no information gain)\n",
    "    if info == 0:\n",
    "        return Leaf(tree)\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "    # recursive call to build true_branch\n",
    "    true_branch = build_tree(true_rows)\n",
    "    false_branch = build_tree(false_rows)\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=''):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
